{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SIEM Security Solution - Complete Integration Notebook\n",
        "\n",
        "## One-Class SVM Anomaly Detection (Recommended by Eng Mariam)\n",
        "\n",
        "This notebook demonstrates the complete SIEM integration pipeline:\n",
        "1. **Data Loading & Exploration** - Load and analyze malware detection datasets\n",
        "2. **Data Preprocessing** - Prepare data for machine learning\n",
        "3. **One-Class SVM Training** - Train anomaly detection model on normal behavior\n",
        "4. **Attack Simulation** - Generate realistic attack scenarios\n",
        "5. **Anomaly Detection** - Detect anomalies in real-time\n",
        "6. **SIEM Integration** - Send alerts to SIEM dashboard\n",
        "\n",
        "---\n",
        "\n",
        "**Note**: This solution uses One-Class SVM as recommended by Eng Mariam for detecting anomalies in large-scale security logs without requiring labeled attack data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.svm import OneClassSVM\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
        "import joblib\n",
        "import os\n",
        "import json\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Set display options\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.max_rows', 100)\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"âœ… Libraries imported successfully!\")\n",
        "print(f\"ðŸ“… Execution Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Enhanced Visualizations Added\n",
        "\n",
        "This notebook now includes comprehensive visualizations for SIEM analysis:\n",
        "\n",
        "1. **Correlation Heatmap** - Feature relationships and dependencies\n",
        "2. **Box Plots** - Feature distribution analysis with statistics\n",
        "3. **Violin Plots** - Detailed distribution shapes and quartiles\n",
        "4. **Normal vs Anomaly Comparisons** - Side-by-side feature analysis\n",
        "5. **Pair Plots** - Multi-dimensional feature relationships\n",
        "6. **Statistics Summary** - Comprehensive statistical comparisons\n",
        "7. **Advanced Alert Visualizations** - SIEM dashboard-ready charts\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Load and Explore Data\n",
        "\n",
        "Load the malware detection datasets (Output1.csv, Output2.csv, Output3.csv) and perform exploratory data analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load datasets\n",
        "print(\"=\" * 80)\n",
        "print(\"Loading Datasets\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Load all three CSV files\n",
        "df1 = pd.read_csv('Output1.csv')\n",
        "df2 = pd.read_csv('output2.csv')\n",
        "df3 = pd.read_csv('output3.csv')\n",
        "\n",
        "print(f\"\\nDataset 1 (Output1.csv): {df1.shape[0]} rows, {df1.shape[1]} columns\")\n",
        "print(f\"Dataset 2 (output2.csv): {df2.shape[0]} rows, {df2.shape[1]} columns\")\n",
        "print(f\"Dataset 3 (output3.csv): {df3.shape[0]} rows, {df3.shape[1]} columns\")\n",
        "\n",
        "# Combine datasets\n",
        "df_combined = pd.concat([df1, df2, df3], ignore_index=True)\n",
        "print(f\"\\nCombined Dataset: {df_combined.shape[0]} rows, {df_combined.shape[1]} columns\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"First 5 Rows\")\n",
        "print(\"=\" * 80)\n",
        "df_combined.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Data Visualizations - Correlation Analysis\n",
        "print(\"=\" * 80)\n",
        "print(\"Feature Correlation Analysis\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select key features for correlation analysis\n",
        "key_features = ['pslist.nproc', 'pslist.avg_threads', 'dlllist.ndlls', \n",
        "                 'handles.nhandles', 'handles.nport', 'handles.nfile',\n",
        "                 'malfind.ninjections', 'pslist.avg_handlers', 'dlllist.avg_dlls_per_proc']\n",
        "\n",
        "# Filter to only include features that exist in the dataframe\n",
        "key_features = [f for f in key_features if f in df_combined.columns]\n",
        "\n",
        "# Calculate correlation matrix\n",
        "corr_matrix = df_combined[key_features].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "fig, ax = plt.subplots(figsize=(14, 12))\n",
        "sns.heatmap(corr_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=1, cbar_kws={\"shrink\": 0.8}, ax=ax)\n",
        "ax.set_title('Feature Correlation Heatmap', fontsize=16, fontweight='bold', pad=20)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… Correlation analysis completed for {len(key_features)} key features!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Box Plots - Comparing Feature Distributions\n",
        "print(\"=\" * 80)\n",
        "print(\"Feature Distribution Comparison (Box Plots)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select features for box plot comparison\n",
        "box_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections']\n",
        "box_features = [f for f in box_features if f in df_combined.columns]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(box_features):\n",
        "    bp = axes[idx].boxplot(df_combined[feature].dropna(), patch_artist=True, \n",
        "                           showmeans=True, meanline=True)\n",
        "    axes[idx].set_title(f'Distribution of {feature}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Color the boxes\n",
        "    for patch in bp['boxes']:\n",
        "        patch.set_facecolor('lightblue')\n",
        "        patch.set_alpha(0.7)\n",
        "    \n",
        "    # Add statistics text\n",
        "    stats = df_combined[feature].describe()\n",
        "    textstr = f'Mean: {stats[\"mean\"]:.2f}\\nMedian: {stats[\"50%\"]:.2f}\\nStd: {stats[\"std\"]:.2f}'\n",
        "    axes[idx].text(0.02, 0.98, textstr, transform=axes[idx].transAxes, \n",
        "                   fontsize=9, verticalalignment='top', \n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Box plot analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Violin Plots - Detailed Distribution Analysis\n",
        "print(\"=\" * 80)\n",
        "print(\"Violin Plots - Detailed Feature Distributions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "violin_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections']\n",
        "violin_features = [f for f in violin_features if f in df_combined.columns]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(violin_features):\n",
        "    # Create violin plot\n",
        "    parts = axes[idx].violinplot([df_combined[feature].dropna()], positions=[1], \n",
        "                                  showmeans=True, showmedians=True)\n",
        "    \n",
        "    # Customize violin plot colors\n",
        "    for pc in parts['bodies']:\n",
        "        pc.set_facecolor('skyblue')\n",
        "        pc.set_alpha(0.7)\n",
        "    \n",
        "    axes[idx].set_title(f'Violin Plot: {feature}', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].set_xticks([1])\n",
        "    axes[idx].set_xticklabels([feature.split('.')[-1]])\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add quartile lines\n",
        "    q1 = df_combined[feature].quantile(0.25)\n",
        "    q2 = df_combined[feature].quantile(0.50)\n",
        "    q3 = df_combined[feature].quantile(0.75)\n",
        "    axes[idx].axhline(y=q1, color='green', linestyle='--', alpha=0.7, label='Q1')\n",
        "    axes[idx].axhline(y=q2, color='red', linestyle='--', alpha=0.7, label='Median')\n",
        "    axes[idx].axhline(y=q3, color='blue', linestyle='--', alpha=0.7, label='Q3')\n",
        "    axes[idx].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Violin plot analysis completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data Information\n",
        "print(\"=\" * 80)\n",
        "print(\"Dataset Information\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nColumn Names ({len(df_combined.columns)} total):\")\n",
        "print(df_combined.columns.tolist())\n",
        "\n",
        "print(f\"\\n\\nData Types:\")\n",
        "print(df_combined.dtypes.value_counts())\n",
        "\n",
        "print(f\"\\n\\nMissing Values:\")\n",
        "missing = df_combined.isnull().sum()\n",
        "missing_pct = (missing / len(df_combined)) * 100\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing Count': missing,\n",
        "    'Percentage': missing_pct\n",
        "})\n",
        "missing_df = missing_df[missing_df['Missing Count'] > 0].sort_values('Missing Count', ascending=False)\n",
        "if len(missing_df) > 0:\n",
        "    print(missing_df)\n",
        "else:\n",
        "    print(\"No missing values found!\")\n",
        "\n",
        "print(f\"\\n\\nBasic Statistics:\")\n",
        "df_combined.describe()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize data distribution\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Distribution of key features\n",
        "axes[0, 0].hist(df_combined['pslist.nproc'], bins=30, edgecolor='black', alpha=0.7)\n",
        "axes[0, 0].set_title('Distribution of Number of Processes', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Number of Processes')\n",
        "axes[0, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 2. Distribution of handles\n",
        "axes[0, 1].hist(df_combined['handles.nhandles'], bins=30, edgecolor='black', alpha=0.7, color='orange')\n",
        "axes[0, 1].set_title('Distribution of Number of Handles', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Number of Handles')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "\n",
        "# 3. Distribution of DLLs\n",
        "axes[1, 0].hist(df_combined['dlllist.ndlls'], bins=30, edgecolor='black', alpha=0.7, color='green')\n",
        "axes[1, 0].set_title('Distribution of Number of DLLs', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].set_xlabel('Number of DLLs')\n",
        "axes[1, 0].set_ylabel('Frequency')\n",
        "\n",
        "# 4. Distribution of malware injections\n",
        "axes[1, 1].hist(df_combined['malfind.ninjections'], bins=30, edgecolor='black', alpha=0.7, color='red')\n",
        "axes[1, 1].set_title('Distribution of Malware Injections', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].set_xlabel('Number of Injections')\n",
        "axes[1, 1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Data visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Data Preprocessing\n",
        "\n",
        "Prepare the data for One-Class SVM training by:\n",
        "- Removing non-numeric columns (Filename)\n",
        "- Handling missing values\n",
        "- Feature scaling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocess data for One-Class SVM\n",
        "print(\"=\" * 80)\n",
        "print(\"Data Preprocessing\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Store filename column for later reference\n",
        "filenames = df_combined['Filename'].copy()\n",
        "\n",
        "# Remove filename column (non-numeric)\n",
        "df_features = df_combined.drop(columns=['Filename'])\n",
        "\n",
        "# Check for any remaining non-numeric columns\n",
        "non_numeric_cols = df_features.select_dtypes(include=['object']).columns.tolist()\n",
        "if non_numeric_cols:\n",
        "    print(f\"âš ï¸ Found non-numeric columns: {non_numeric_cols}\")\n",
        "    df_features = df_features.drop(columns=non_numeric_cols)\n",
        "\n",
        "print(f\"\\nFeatures shape: {df_features.shape}\")\n",
        "print(f\"Feature columns: {len(df_features.columns)}\")\n",
        "\n",
        "# Handle missing values (fill with median)\n",
        "if df_features.isnull().sum().sum() > 0:\n",
        "    print(f\"\\nFilling {df_features.isnull().sum().sum()} missing values with median...\")\n",
        "    df_features = df_features.fillna(df_features.median())\n",
        "else:\n",
        "    print(\"\\nâœ… No missing values found!\")\n",
        "\n",
        "# Check for infinite values\n",
        "inf_count = np.isinf(df_features.select_dtypes(include=[np.number])).sum().sum()\n",
        "if inf_count > 0:\n",
        "    print(f\"\\nâš ï¸ Found {inf_count} infinite values. Replacing with NaN then median...\")\n",
        "    df_features = df_features.replace([np.inf, -np.inf], np.nan)\n",
        "    df_features = df_features.fillna(df_features.median())\n",
        "\n",
        "print(f\"\\nâœ… Preprocessing complete! Final shape: {df_features.shape}\")\n",
        "print(f\"Feature names: {list(df_features.columns[:10])}...\" if len(df_features.columns) > 10 else f\"Feature names: {list(df_features.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Train One-Class SVM Model\n",
        "\n",
        "Train One-Class SVM model on the data. For anomaly detection, we'll use the entire dataset as \"normal\" behavior and the model will learn to identify deviations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Anomaly Visualizations - Box Plots Comparing Normal vs Anomaly\n",
        "print(\"=\" * 80)\n",
        "print(\"Feature Comparison: Normal vs Anomaly (Box Plots)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select key features for comparison\n",
        "compare_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections']\n",
        "compare_features = [f for f in compare_features if f in results_df.columns]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(compare_features):\n",
        "    # Separate normal and anomaly data\n",
        "    normal_data = results_df[results_df['Is_Anomaly'] == 0][feature].dropna()\n",
        "    anomaly_data = results_df[results_df['Is_Anomaly'] == 1][feature].dropna()\n",
        "    \n",
        "    # Create box plot\n",
        "    bp = axes[idx].boxplot([normal_data, anomaly_data], \n",
        "                           labels=['Normal', 'Anomaly'],\n",
        "                           patch_artist=True, showmeans=True, meanline=True)\n",
        "    \n",
        "    # Color the boxes\n",
        "    colors = ['lightgreen', 'lightcoral']\n",
        "    for patch, color in zip(bp['boxes'], colors):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "    \n",
        "    axes[idx].set_title(f'{feature}: Normal vs Anomaly', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add statistics\n",
        "    normal_mean = normal_data.mean()\n",
        "    anomaly_mean = anomaly_data.mean()\n",
        "    textstr = f'Normal Mean: {normal_mean:.2f}\\nAnomaly Mean: {anomaly_mean:.2f}\\nDifference: {abs(anomaly_mean - normal_mean):.2f}'\n",
        "    axes[idx].text(0.02, 0.98, textstr, transform=axes[idx].transAxes, \n",
        "                   fontsize=9, verticalalignment='top',\n",
        "                   bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Normal vs Anomaly comparison completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Violin Plots - Normal vs Anomaly Distribution Comparison\n",
        "print(\"=\" * 80)\n",
        "print(\"Violin Plots: Normal vs Anomaly Distributions\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "violin_compare_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections']\n",
        "violin_compare_features = [f for f in violin_compare_features if f in results_df.columns]\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.flatten()\n",
        "\n",
        "for idx, feature in enumerate(violin_compare_features):\n",
        "    # Separate normal and anomaly data\n",
        "    normal_data = results_df[results_df['Is_Anomaly'] == 0][feature].dropna()\n",
        "    anomaly_data = results_df[results_df['Is_Anomaly'] == 1][feature].dropna()\n",
        "    \n",
        "    # Create violin plot\n",
        "    parts = axes[idx].violinplot([normal_data, anomaly_data], positions=[1, 2],\n",
        "                                 showmeans=True, showmedians=True)\n",
        "    \n",
        "    # Customize colors\n",
        "    colors = ['lightgreen', 'lightcoral']\n",
        "    for pc, color in zip(parts['bodies'], colors):\n",
        "        pc.set_facecolor(color)\n",
        "        pc.set_alpha(0.7)\n",
        "    \n",
        "    axes[idx].set_title(f'{feature}: Distribution Comparison', fontsize=12, fontweight='bold')\n",
        "    axes[idx].set_ylabel('Value')\n",
        "    axes[idx].set_xticks([1, 2])\n",
        "    axes[idx].set_xticklabels(['Normal', 'Anomaly'])\n",
        "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
        "    \n",
        "    # Add mean lines\n",
        "    axes[idx].axhline(y=normal_data.mean(), xmin=0, xmax=0.5, color='green', \n",
        "                     linestyle='--', alpha=0.7, linewidth=2, label='Normal Mean')\n",
        "    axes[idx].axhline(y=anomaly_data.mean(), xmin=0.5, xmax=1, color='red', \n",
        "                     linestyle='--', alpha=0.7, linewidth=2, label='Anomaly Mean')\n",
        "    axes[idx].legend(loc='upper right', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Violin plot comparison completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pair Plot - Multi-dimensional Feature Relationships\n",
        "print(\"=\" * 80)\n",
        "print(\"Pair Plot Analysis - Feature Relationships\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Select top features for pair plot (limit to 4-5 features for readability)\n",
        "pair_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections']\n",
        "pair_features = [f for f in pair_features if f in results_df.columns]\n",
        "\n",
        "# Sample data if too large for pair plot (for performance)\n",
        "sample_size = min(1000, len(results_df))\n",
        "df_sample = results_df.sample(n=sample_size, random_state=42) if len(results_df) > sample_size else results_df\n",
        "\n",
        "# Create pair plot with hue based on anomaly status\n",
        "pair_df = df_sample[pair_features + ['Is_Anomaly']].copy()\n",
        "\n",
        "fig, axes = plt.subplots(len(pair_features), len(pair_features), figsize=(16, 16))\n",
        "\n",
        "for i, feat1 in enumerate(pair_features):\n",
        "    for j, feat2 in enumerate(pair_features):\n",
        "        ax = axes[i, j]\n",
        "        \n",
        "        if i == j:\n",
        "            # Diagonal: histogram\n",
        "            normal_data = pair_df[pair_df['Is_Anomaly'] == 0][feat1].dropna()\n",
        "            anomaly_data = pair_df[pair_df['Is_Anomaly'] == 1][feat1].dropna()\n",
        "            ax.hist(normal_data, bins=20, alpha=0.6, color='green', label='Normal', edgecolor='black')\n",
        "            ax.hist(anomaly_data, bins=20, alpha=0.6, color='red', label='Anomaly', edgecolor='black')\n",
        "            ax.set_ylabel('Frequency')\n",
        "        else:\n",
        "            # Off-diagonal: scatter plot\n",
        "            normal_mask = pair_df['Is_Anomaly'] == 0\n",
        "            anomaly_mask = pair_df['Is_Anomaly'] == 1\n",
        "            \n",
        "            ax.scatter(pair_df.loc[normal_mask, feat2], pair_df.loc[normal_mask, feat1],\n",
        "                      alpha=0.5, c='green', s=20, label='Normal', edgecolors='black', linewidths=0.5)\n",
        "            ax.scatter(pair_df.loc[anomaly_mask, feat2], pair_df.loc[anomaly_mask, feat1],\n",
        "                      alpha=0.5, c='red', s=20, label='Anomaly', edgecolors='black', linewidths=0.5)\n",
        "        \n",
        "        # Set labels\n",
        "        if i == len(pair_features) - 1:\n",
        "            ax.set_xlabel(feat2.split('.')[-1], fontsize=9)\n",
        "        if j == 0:\n",
        "            ax.set_ylabel(feat1.split('.')[-1], fontsize=9)\n",
        "        \n",
        "        # Remove ticks for cleaner look\n",
        "        ax.tick_params(labelsize=7)\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "plt.suptitle('Pair Plot: Feature Relationships (Normal vs Anomaly)', \n",
        "             fontsize=16, fontweight='bold', y=0.995)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"âœ… Pair plot analysis completed! (Sample size: {len(df_sample)} rows)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Statistics Summary Visualization\n",
        "print(\"=\" * 80)\n",
        "print(\"Feature Statistics Summary\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate statistics for key features\n",
        "key_stats_features = ['pslist.nproc', 'handles.nhandles', 'dlllist.ndlls', 'malfind.ninjections',\n",
        "                      'pslist.avg_threads', 'handles.nport', 'handles.nfile']\n",
        "key_stats_features = [f for f in key_stats_features if f in results_df.columns]\n",
        "\n",
        "# Create summary statistics dataframe\n",
        "stats_summary = []\n",
        "for feature in key_stats_features:\n",
        "    normal_data = results_df[results_df['Is_Anomaly'] == 0][feature].dropna()\n",
        "    anomaly_data = results_df[results_df['Is_Anomaly'] == 1][feature].dropna()\n",
        "    \n",
        "    stats_summary.append({\n",
        "        'Feature': feature.split('.')[-1],\n",
        "        'Normal_Mean': normal_data.mean(),\n",
        "        'Normal_Std': normal_data.std(),\n",
        "        'Anomaly_Mean': anomaly_data.mean(),\n",
        "        'Anomaly_Std': anomaly_data.std(),\n",
        "        'Difference': abs(anomaly_data.mean() - normal_data.mean()),\n",
        "        'Normal_Median': normal_data.median(),\n",
        "        'Anomaly_Median': anomaly_data.median()\n",
        "    })\n",
        "\n",
        "stats_df = pd.DataFrame(stats_summary)\n",
        "\n",
        "# Visualize statistics comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Mean comparison\n",
        "x_pos = np.arange(len(stats_df))\n",
        "width = 0.35\n",
        "axes[0, 0].bar(x_pos - width/2, stats_df['Normal_Mean'], width, label='Normal', \n",
        "               color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].bar(x_pos + width/2, stats_df['Anomaly_Mean'], width, label='Anomaly', \n",
        "               color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_xlabel('Features')\n",
        "axes[0, 0].set_ylabel('Mean Value')\n",
        "axes[0, 0].set_title('Mean Comparison: Normal vs Anomaly', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].set_xticks(x_pos)\n",
        "axes[0, 0].set_xticklabels(stats_df['Feature'], rotation=45, ha='right')\n",
        "axes[0, 0].legend()\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 2. Standard deviation comparison\n",
        "axes[0, 1].bar(x_pos - width/2, stats_df['Normal_Std'], width, label='Normal', \n",
        "               color='lightblue', alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].bar(x_pos + width/2, stats_df['Anomaly_Std'], width, label='Anomaly', \n",
        "               color='orange', alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_xlabel('Features')\n",
        "axes[0, 1].set_ylabel('Standard Deviation')\n",
        "axes[0, 1].set_title('Std Dev Comparison: Normal vs Anomaly', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].set_xticks(x_pos)\n",
        "axes[0, 1].set_xticklabels(stats_df['Feature'], rotation=45, ha='right')\n",
        "axes[0, 1].legend()\n",
        "axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Difference in means\n",
        "axes[1, 0].barh(range(len(stats_df)), stats_df['Difference'], \n",
        "                color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[1, 0].set_yticks(range(len(stats_df)))\n",
        "axes[1, 0].set_yticklabels(stats_df['Feature'])\n",
        "axes[1, 0].set_xlabel('Absolute Difference in Means')\n",
        "axes[1, 0].set_title('Feature Discriminative Power (Difference in Means)', \n",
        "                     fontsize=12, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "# 4. Median comparison\n",
        "axes[1, 1].bar(x_pos - width/2, stats_df['Normal_Median'], width, label='Normal', \n",
        "               color='lightgreen', alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].bar(x_pos + width/2, stats_df['Anomaly_Median'], width, label='Anomaly', \n",
        "               color='lightcoral', alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].set_xlabel('Features')\n",
        "axes[1, 1].set_ylabel('Median Value')\n",
        "axes[1, 1].set_title('Median Comparison: Normal vs Anomaly', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].set_xticks(x_pos)\n",
        "axes[1, 1].set_xticklabels(stats_df['Feature'], rotation=45, ha='right')\n",
        "axes[1, 1].legend()\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Display statistics table\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Detailed Statistics Summary\")\n",
        "print(\"=\" * 80)\n",
        "print(stats_df.to_string(index=False))\n",
        "\n",
        "print(\"\\nâœ… Statistics summary visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Split data for training and testing\n",
        "# For One-Class SVM, we train on all data to learn normal patterns\n",
        "# Then we can use it to detect anomalies\n",
        "\n",
        "X = df_features.values\n",
        "print(f\"Data shape: {X.shape}\")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(\"âœ… Features scaled successfully!\")\n",
        "print(f\"Scaled data shape: {X_scaled.shape}\")\n",
        "print(f\"Mean: {X_scaled.mean():.6f}, Std: {X_scaled.std():.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train One-Class SVM Model\n",
        "print(\"=\" * 80)\n",
        "print(\"Training One-Class SVM Model\")\n",
        "print(\"=\" * 80)\n",
        "print(\"(Recommended by Eng Mariam for anomaly detection)\")\n",
        "\n",
        "# Model parameters\n",
        "nu = 0.1  # Expected fraction of outliers (10%)\n",
        "gamma = 'scale'  # Kernel coefficient\n",
        "kernel = 'rbf'  # Radial Basis Function kernel\n",
        "\n",
        "print(f\"\\nModel Parameters:\")\n",
        "print(f\"  nu (outlier fraction): {nu}\")\n",
        "print(f\"  gamma: {gamma}\")\n",
        "print(f\"  kernel: {kernel}\")\n",
        "\n",
        "# Initialize and train model\n",
        "ocsvm_model = OneClassSVM(\n",
        "    nu=nu,\n",
        "    gamma=gamma,\n",
        "    kernel=kernel,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTraining model... (this may take a few minutes)\")\n",
        "ocsvm_model.fit(X_scaled)\n",
        "\n",
        "print(\"âœ… Model training completed!\")\n",
        "\n",
        "# Get predictions on training data\n",
        "train_predictions = ocsvm_model.predict(X_scaled)\n",
        "train_scores = ocsvm_model.decision_function(X_scaled)\n",
        "\n",
        "# Statistics\n",
        "n_normal = (train_predictions == 1).sum()\n",
        "n_anomaly = (train_predictions == -1).sum()\n",
        "\n",
        "print(f\"\\nModel Statistics:\")\n",
        "print(f\"  Support vectors: {ocsvm_model.n_support_[0]}\")\n",
        "print(f\"  Samples predicted as normal: {n_normal} ({n_normal/len(train_predictions)*100:.2f}%)\")\n",
        "print(f\"  Samples predicted as anomaly: {n_anomaly} ({n_anomaly/len(train_predictions)*100:.2f}%)\")\n",
        "print(f\"  Decision score range: [{train_scores.min():.2f}, {train_scores.max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize model predictions\n",
        "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# 1. Decision scores distribution\n",
        "axes[0].hist(train_scores, bins=50, edgecolor='black', alpha=0.7, color='skyblue')\n",
        "axes[0].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Decision Boundary')\n",
        "axes[0].set_title('Distribution of Decision Scores', fontsize=14, fontweight='bold')\n",
        "axes[0].set_xlabel('Decision Score')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "axes[0].legend()\n",
        "axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Predictions distribution\n",
        "pred_counts = pd.Series(train_predictions).value_counts().sort_index()\n",
        "axes[1].bar(['Anomaly (-1)', 'Normal (1)'], [pred_counts.get(-1, 0), pred_counts.get(1, 0)], \n",
        "            color=['red', 'green'], alpha=0.7, edgecolor='black')\n",
        "axes[1].set_title('Model Predictions Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Model visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive SIEM Dashboard Summary View\n",
        "print(\"=\" * 80)\n",
        "print(\"SIEM Dashboard - Comprehensive Summary View\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create a comprehensive dashboard figure\n",
        "fig = plt.figure(figsize=(22, 14))\n",
        "gs = fig.add_gridspec(3, 4, hspace=0.3, wspace=0.3)\n",
        "\n",
        "# 1. Alert Severity Distribution (Top Left - Large)\n",
        "ax1 = fig.add_subplot(gs[0, 0:2])\n",
        "severity_counts = alerts_df['severity'].value_counts().reindex(['critical', 'high', 'medium', 'low'], fill_value=0)\n",
        "colors = {'critical': 'darkred', 'high': 'red', 'medium': 'orange', 'low': 'yellow'}\n",
        "bars = ax1.bar(severity_counts.index, severity_counts.values, \n",
        "               color=[colors[s] for s in severity_counts.index], alpha=0.8, edgecolor='black', linewidth=2)\n",
        "ax1.set_title('Alert Severity Distribution', fontsize=14, fontweight='bold', pad=15)\n",
        "ax1.set_xlabel('Severity Level', fontsize=12)\n",
        "ax1.set_ylabel('Number of Alerts', fontsize=12)\n",
        "ax1.grid(True, alpha=0.3, axis='y')\n",
        "for i, (bar, v) in enumerate(zip(bars, severity_counts.values)):\n",
        "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5, str(v),\n",
        "            ha='center', va='bottom', fontweight='bold', fontsize=11)\n",
        "\n",
        "# 2. Anomaly Score Distribution (Top Right)\n",
        "ax2 = fig.add_subplot(gs[0, 2:])\n",
        "ax2.hist(alerts_df['anomaly_score'], bins=25, color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "ax2.axvline(alerts_df['anomaly_score'].mean(), color='red', linestyle='--', linewidth=2, label=f'Mean: {alerts_df[\"anomaly_score\"].mean():.2f}')\n",
        "ax2.axvline(alerts_df['anomaly_score'].median(), color='green', linestyle='--', linewidth=2, label=f'Median: {alerts_df[\"anomaly_score\"].median():.2f}')\n",
        "ax2.set_title('Anomaly Score Distribution', fontsize=14, fontweight='bold', pad=15)\n",
        "ax2.set_xlabel('Anomaly Score', fontsize=12)\n",
        "ax2.set_ylabel('Frequency', fontsize=12)\n",
        "ax2.legend(fontsize=10)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Key Indicators Comparison (Middle Left)\n",
        "ax3 = fig.add_subplot(gs[1, 0:2])\n",
        "if len(key_indicators_df.columns) > 0:\n",
        "    indicator_totals = key_indicators_df.sum().sort_values(ascending=True).tail(8)\n",
        "    ax3.barh(range(len(indicator_totals)), indicator_totals.values, \n",
        "            color='coral', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax3.set_yticks(range(len(indicator_totals)))\n",
        "    ax3.set_yticklabels([col.replace('_', ' ').title() for col in indicator_totals.index], fontsize=10)\n",
        "    ax3.set_xlabel('Total Count', fontsize=11)\n",
        "    ax3.set_title('Top Key Indicators in Alerts', fontsize=13, fontweight='bold', pad=10)\n",
        "    ax3.grid(True, alpha=0.3, axis='x')\n",
        "    for i, v in enumerate(indicator_totals.values):\n",
        "        ax3.text(v + max(indicator_totals.values) * 0.01, i, f'{int(v)}', \n",
        "                va='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# 4. Severity vs Key Metrics (Middle Right)\n",
        "ax4 = fig.add_subplot(gs[1, 2:])\n",
        "if 'num_processes' in alerts_extended.columns and 'num_handles' in alerts_extended.columns:\n",
        "    scatter = ax4.scatter(alerts_extended['num_processes'], alerts_extended['num_handles'],\n",
        "                        c=[severity_order.get(s, 3) for s in alerts_df['severity']],\n",
        "                        cmap='RdYlGn_r', s=120, alpha=0.7, edgecolors='black', linewidth=1)\n",
        "    ax4.set_xlabel('Number of Processes', fontsize=11)\n",
        "    ax4.set_ylabel('Number of Handles', fontsize=11)\n",
        "    ax4.set_title('Processes vs Handles (Colored by Severity)', fontsize=13, fontweight='bold', pad=10)\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    cbar = plt.colorbar(scatter, ax=ax4)\n",
        "    cbar.set_label('Severity Level', fontsize=10)\n",
        "\n",
        "# 5. Decision Score Analysis (Bottom Left)\n",
        "ax5 = fig.add_subplot(gs[2, 0])\n",
        "normal_decisions = results_df[results_df['Is_Anomaly'] == 0]['Decision_Score']\n",
        "anomaly_decisions = results_df[results_df['Is_Anomaly'] == 1]['Decision_Score']\n",
        "ax5.hist(normal_decisions, bins=25, alpha=0.6, label='Normal', color='green', edgecolor='black', linewidth=1)\n",
        "ax5.hist(anomaly_decisions, bins=25, alpha=0.6, label='Anomaly', color='red', edgecolor='black', linewidth=1)\n",
        "ax5.axvline(x=0, color='blue', linestyle='--', linewidth=2, label='Decision Boundary')\n",
        "ax5.set_xlabel('Decision Score', fontsize=10)\n",
        "ax5.set_ylabel('Frequency', fontsize=10)\n",
        "ax5.set_title('Decision Score Distribution', fontsize=12, fontweight='bold')\n",
        "ax5.legend(fontsize=9)\n",
        "ax5.grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Alert Statistics Summary (Bottom Middle)\n",
        "ax6 = fig.add_subplot(gs[2, 1])\n",
        "stats_text = f\"\"\"\n",
        "Total Alerts: {len(alerts_df)}\n",
        "Critical: {len(alerts_df[alerts_df['severity'] == 'critical'])}\n",
        "High: {len(alerts_df[alerts_df['severity'] == 'high'])}\n",
        "Medium: {len(alerts_df[alerts_df['severity'] == 'medium'])}\n",
        "Low: {len(alerts_df[alerts_df['severity'] == 'low'])}\n",
        "\n",
        "Avg Anomaly Score: {alerts_df['anomaly_score'].mean():.2f}\n",
        "Max Anomaly Score: {alerts_df['anomaly_score'].max():.2f}\n",
        "Min Anomaly Score: {alerts_df['anomaly_score'].min():.2f}\n",
        "\n",
        "Detection Rate: {(results_df['Is_Anomaly'].sum() / len(results_df) * 100):.2f}%\n",
        "\"\"\"\n",
        "ax6.text(0.1, 0.5, stats_text, transform=ax6.transAxes, fontsize=11,\n",
        "         verticalalignment='center', family='monospace',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
        "ax6.set_title('Alert Statistics Summary', fontsize=12, fontweight='bold')\n",
        "ax6.axis('off')\n",
        "\n",
        "# 7. Feature Correlation with Anomaly (Bottom Right - 2 columns)\n",
        "ax7 = fig.add_subplot(gs[2, 2:])\n",
        "if len(key_indicators_df.columns) > 0:\n",
        "    # Calculate correlation between indicators and anomaly score\n",
        "    indicator_corr = key_indicators_df.corrwith(alerts_df['anomaly_score']).abs().sort_values(ascending=True).tail(6)\n",
        "    ax7.barh(range(len(indicator_corr)), indicator_corr.values, \n",
        "            color='steelblue', alpha=0.7, edgecolor='black', linewidth=1.5)\n",
        "    ax7.set_yticks(range(len(indicator_corr)))\n",
        "    ax7.set_yticklabels([col.replace('_', ' ').title() for col in indicator_corr.index], fontsize=9)\n",
        "    ax7.set_xlabel('Correlation with Anomaly Score', fontsize=10)\n",
        "    ax7.set_title('Top Indicators Correlated with Anomaly Score', fontsize=12, fontweight='bold')\n",
        "    ax7.grid(True, alpha=0.3, axis='x')\n",
        "    for i, v in enumerate(indicator_corr.values):\n",
        "        ax7.text(v + max(indicator_corr.values) * 0.02, i, f'{v:.3f}', \n",
        "                va='center', fontweight='bold', fontsize=9)\n",
        "\n",
        "# Add main title\n",
        "fig.suptitle('SIEM Security Dashboard - Comprehensive Analysis', \n",
        "             fontsize=18, fontweight='bold', y=0.995)\n",
        "\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Comprehensive SIEM dashboard visualization completed!\")\n",
        "print(f\"\\nDashboard Summary:\")\n",
        "print(f\"  - Total Alerts: {len(alerts_df)}\")\n",
        "print(f\"  - Critical Alerts: {len(alerts_df[alerts_df['severity'] == 'critical'])}\")\n",
        "print(f\"  - High Alerts: {len(alerts_df[alerts_df['severity'] == 'high'])}\")\n",
        "print(f\"  - Average Anomaly Score: {alerts_df['anomaly_score'].mean():.2f}\")\n",
        "print(f\"  - Detection Rate: {(results_df['Is_Anomaly'].sum() / len(results_df) * 100):.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced Alert Visualizations - Comprehensive SIEM Dashboard Views\n",
        "print(\"=\" * 80)\n",
        "print(\"Advanced Alert Visualizations for SIEM Dashboard\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Extract key indicators from alerts\n",
        "alerts_df = pd.DataFrame(alerts)\n",
        "key_indicators_df = pd.json_normalize(alerts_df['key_indicators'])\n",
        "alerts_extended = pd.concat([alerts_df[['severity', 'anomaly_score', 'decision_score']], \n",
        "                            key_indicators_df], axis=1)\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
        "\n",
        "# 1. Severity pie chart\n",
        "severity_counts = alerts_df['severity'].value_counts()\n",
        "colors_pie = {'critical': 'darkred', 'high': 'red', 'medium': 'orange', 'low': 'yellow'}\n",
        "pie_colors = [colors_pie.get(s, 'gray') for s in severity_counts.index]\n",
        "axes[0, 0].pie(severity_counts.values, labels=severity_counts.index, autopct='%1.1f%%',\n",
        "               colors=pie_colors, startangle=90, textprops={'fontsize': 10, 'fontweight': 'bold'})\n",
        "axes[0, 0].set_title('Alert Severity Distribution (Pie Chart)', fontsize=12, fontweight='bold')\n",
        "\n",
        "# 2. Anomaly score by severity (box plot)\n",
        "severity_order_list = ['critical', 'high', 'medium', 'low']\n",
        "severity_data = [alerts_df[alerts_df['severity'] == s]['anomaly_score'].values \n",
        "                 for s in severity_order_list if s in alerts_df['severity'].values]\n",
        "severity_labels = [s for s in severity_order_list if s in alerts_df['severity'].values]\n",
        "\n",
        "if severity_data:\n",
        "    bp = axes[0, 1].boxplot(severity_data, labels=severity_labels, patch_artist=True, showmeans=True)\n",
        "    colors_box = [colors_pie.get(s, 'gray') for s in severity_labels]\n",
        "    for patch, color in zip(bp['boxes'], colors_box):\n",
        "        patch.set_facecolor(color)\n",
        "        patch.set_alpha(0.7)\n",
        "    axes[0, 1].set_title('Anomaly Score Distribution by Severity', fontsize=12, fontweight='bold')\n",
        "    axes[0, 1].set_ylabel('Anomaly Score')\n",
        "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 3. Key indicators heatmap\n",
        "if len(key_indicators_df.columns) > 0:\n",
        "    indicator_means = key_indicators_df.groupby(alerts_df['severity']).mean()\n",
        "    sns.heatmap(indicator_means.T, annot=True, fmt='.2f', cmap='YlOrRd', \n",
        "                ax=axes[0, 2], cbar_kws={\"shrink\": 0.8})\n",
        "    axes[0, 2].set_title('Average Key Indicators by Severity', fontsize=12, fontweight='bold')\n",
        "    axes[0, 2].set_xlabel('Severity')\n",
        "    axes[0, 2].set_ylabel('Key Indicators')\n",
        "\n",
        "# 4. Scatter: Processes vs Handles colored by severity\n",
        "if 'num_processes' in alerts_extended.columns and 'num_handles' in alerts_extended.columns:\n",
        "    scatter = axes[1, 0].scatter(alerts_extended['num_processes'], alerts_extended['num_handles'],\n",
        "                                 c=[severity_order.get(s, 3) for s in alerts_df['severity']],\n",
        "                                 cmap='RdYlGn_r', s=100, alpha=0.6, edgecolors='black')\n",
        "    axes[1, 0].set_xlabel('Number of Processes', fontsize=11)\n",
        "    axes[1, 0].set_ylabel('Number of Handles', fontsize=11)\n",
        "    axes[1, 0].set_title('Processes vs Handles (by Severity)', fontsize=12, fontweight='bold')\n",
        "    axes[1, 0].grid(True, alpha=0.3)\n",
        "    cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
        "    cbar.set_label('Severity')\n",
        "\n",
        "# 5. Stacked bar chart: Key indicators by severity\n",
        "if len(key_indicators_df.columns) > 0:\n",
        "    indicator_by_severity = key_indicators_df.groupby(alerts_df['severity']).sum()\n",
        "    indicator_by_severity = indicator_by_severity.reindex(severity_order_list, fill_value=0)\n",
        "    \n",
        "    indicator_by_severity.plot(kind='bar', stacked=True, ax=axes[1, 1], \n",
        "                              color=['skyblue', 'lightcoral', 'lightgreen', 'gold', 'plum'][:len(indicator_by_severity.columns)],\n",
        "                              edgecolor='black', alpha=0.7)\n",
        "    axes[1, 1].set_title('Key Indicators Sum by Severity (Stacked)', fontsize=12, fontweight='bold')\n",
        "    axes[1, 1].set_xlabel('Severity')\n",
        "    axes[1, 1].set_ylabel('Total Count')\n",
        "    axes[1, 1].legend(title='Indicators', fontsize=8, title_fontsize=9)\n",
        "    axes[1, 1].tick_params(axis='x', rotation=0)\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# 6. Decision score vs Anomaly score\n",
        "scatter = axes[1, 2].scatter(alerts_df['decision_score'], alerts_df['anomaly_score'],\n",
        "                            c=[severity_order.get(s, 3) for s in alerts_df['severity']],\n",
        "                            cmap='RdYlGn_r', s=100, alpha=0.6, edgecolors='black')\n",
        "axes[1, 2].set_xlabel('Decision Score', fontsize=11)\n",
        "axes[1, 2].set_ylabel('Anomaly Score', fontsize=11)\n",
        "axes[1, 2].set_title('Decision Score vs Anomaly Score', fontsize=12, fontweight='bold')\n",
        "axes[1, 2].grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(scatter, ax=axes[1, 2])\n",
        "cbar.set_label('Severity')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Advanced alert visualizations completed!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save model and scaler\n",
        "model_dir = 'models'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "model_path = os.path.join(model_dir, 'one_class_svm_model.pkl')\n",
        "scaler_path = os.path.join(model_dir, 'standard_scaler.pkl')\n",
        "feature_names_path = os.path.join(model_dir, 'feature_names.json')\n",
        "\n",
        "joblib.dump(ocsvm_model, model_path)\n",
        "joblib.dump(scaler, scaler_path)\n",
        "\n",
        "# Save feature names\n",
        "feature_names = df_features.columns.tolist()\n",
        "with open(feature_names_path, 'w') as f:\n",
        "    json.dump(feature_names, f)\n",
        "\n",
        "print(\"âœ… Model and scaler saved successfully!\")\n",
        "print(f\"  Model: {model_path}\")\n",
        "print(f\"  Scaler: {scaler_path}\")\n",
        "print(f\"  Feature names: {feature_names_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Anomaly Detection on New Data\n",
        "\n",
        "Detect anomalies in the dataset and identify potential security threats.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detect anomalies\n",
        "print(\"=\" * 80)\n",
        "print(\"Anomaly Detection\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Get predictions and scores\n",
        "predictions = ocsvm_model.predict(X_scaled)\n",
        "decision_scores = ocsvm_model.decision_function(X_scaled)\n",
        "\n",
        "# Create results dataframe\n",
        "results_df = pd.DataFrame({\n",
        "    'Filename': filenames,\n",
        "    'Prediction': predictions,\n",
        "    'Decision_Score': decision_scores,\n",
        "    'Is_Anomaly': (predictions == -1).astype(int)\n",
        "})\n",
        "\n",
        "# Add original features for analysis\n",
        "results_df = pd.concat([results_df, df_features.reset_index(drop=True)], axis=1)\n",
        "\n",
        "print(f\"\\nDetection Results:\")\n",
        "print(f\"  Total samples: {len(results_df)}\")\n",
        "print(f\"  Normal samples: {(results_df['Is_Anomaly'] == 0).sum()} ({(results_df['Is_Anomaly'] == 0).sum()/len(results_df)*100:.2f}%)\")\n",
        "print(f\"  Anomaly samples: {(results_df['Is_Anomaly'] == 1).sum()} ({(results_df['Is_Anomaly'] == 1).sum()/len(results_df)*100:.2f}%)\")\n",
        "\n",
        "# Display top anomalies (lowest decision scores)\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"Top 10 Anomalies (Lowest Decision Scores)\")\n",
        "print(\"=\" * 80)\n",
        "top_anomalies = results_df.nsmallest(10, 'Decision_Score')[['Filename', 'Decision_Score', 'Is_Anomaly', \n",
        "                                                             'malfind.ninjections', 'pslist.nproc', 'handles.nhandles']]\n",
        "print(top_anomalies.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize anomalies\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Decision scores vs Malware injections\n",
        "scatter = axes[0, 0].scatter(results_df['malfind.ninjections'], results_df['Decision_Score'], \n",
        "                             c=results_df['Is_Anomaly'], cmap='RdYlGn', alpha=0.6, edgecolors='black')\n",
        "axes[0, 0].set_xlabel('Malware Injections', fontsize=12)\n",
        "axes[0, 0].set_ylabel('Decision Score', fontsize=12)\n",
        "axes[0, 0].set_title('Decision Score vs Malware Injections', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=axes[0, 0], label='Anomaly (1) / Normal (0)')\n",
        "\n",
        "# 2. Decision scores vs Number of processes\n",
        "scatter = axes[0, 1].scatter(results_df['pslist.nproc'], results_df['Decision_Score'], \n",
        "                             c=results_df['Is_Anomaly'], cmap='RdYlGn', alpha=0.6, edgecolors='black')\n",
        "axes[0, 1].set_xlabel('Number of Processes', fontsize=12)\n",
        "axes[0, 1].set_ylabel('Decision Score', fontsize=12)\n",
        "axes[0, 1].set_title('Decision Score vs Number of Processes', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "plt.colorbar(scatter, ax=axes[0, 1], label='Anomaly (1) / Normal (0)')\n",
        "\n",
        "# 3. Decision scores distribution by anomaly status\n",
        "normal_scores = results_df[results_df['Is_Anomaly'] == 0]['Decision_Score']\n",
        "anomaly_scores = results_df[results_df['Is_Anomaly'] == 1]['Decision_Score']\n",
        "\n",
        "axes[1, 0].hist(normal_scores, bins=30, alpha=0.7, label='Normal', color='green', edgecolor='black')\n",
        "axes[1, 0].hist(anomaly_scores, bins=30, alpha=0.7, label='Anomaly', color='red', edgecolor='black')\n",
        "axes[1, 0].axvline(x=0, color='blue', linestyle='--', linewidth=2, label='Decision Boundary')\n",
        "axes[1, 0].set_xlabel('Decision Score', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Frequency', fontsize=12)\n",
        "axes[1, 0].set_title('Decision Score Distribution', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].legend()\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Feature importance (correlation with decision scores)\n",
        "feature_corr = df_features.corrwith(results_df['Decision_Score']).abs().sort_values(ascending=False).head(10)\n",
        "axes[1, 1].barh(range(len(feature_corr)), feature_corr.values, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[1, 1].set_yticks(range(len(feature_corr)))\n",
        "axes[1, 1].set_yticklabels(feature_corr.index, fontsize=9)\n",
        "axes[1, 1].set_xlabel('Absolute Correlation with Decision Score', fontsize=12)\n",
        "axes[1, 1].set_title('Top 10 Features Correlated with Anomaly Score', fontsize=14, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Anomaly visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: SIEM Integration - Generate Alerts\n",
        "\n",
        "Create SIEM alerts for detected anomalies with severity levels and recommendations.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate SIEM Alerts\n",
        "print(\"=\" * 80)\n",
        "print(\"SIEM Alert Generation\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def calculate_severity(decision_score):\n",
        "    \"\"\"Calculate alert severity based on decision score\"\"\"\n",
        "    score = abs(decision_score)\n",
        "    if score > 10:\n",
        "        return 'critical'\n",
        "    elif score > 5:\n",
        "        return 'high'\n",
        "    elif score > 2:\n",
        "        return 'medium'\n",
        "    else:\n",
        "        return 'low'\n",
        "\n",
        "def generate_recommendations(row):\n",
        "    \"\"\"Generate security recommendations based on detected anomalies\"\"\"\n",
        "    recommendations = []\n",
        "    \n",
        "    if row['malfind.ninjections'] > 0:\n",
        "        recommendations.append(\"Investigate malware injection - check for code injection attacks\")\n",
        "    \n",
        "    if row['psxview.not_in_pslist'] > 0:\n",
        "        recommendations.append(\"Hidden process detected - investigate rootkit activity\")\n",
        "    \n",
        "    if row['handles.nhandles'] > row['handles.nhandles'].quantile(0.95):\n",
        "        recommendations.append(\"Unusually high number of handles - possible resource exhaustion attack\")\n",
        "    \n",
        "    if row['dlllist.ndlls'] > row['dlllist.ndlls'].quantile(0.95):\n",
        "        recommendations.append(\"High DLL count - check for DLL hijacking or injection\")\n",
        "    \n",
        "    if not recommendations:\n",
        "        recommendations.append(\"Investigate anomalous system behavior\")\n",
        "    \n",
        "    return recommendations\n",
        "\n",
        "# Generate alerts for anomalies only\n",
        "anomalies = results_df[results_df['Is_Anomaly'] == 1].copy()\n",
        "\n",
        "alerts = []\n",
        "for idx, row in anomalies.iterrows():\n",
        "    alert = {\n",
        "        'alert_id': f\"ALERT-{datetime.now().strftime('%Y%m%d%H%M%S')}-{idx}\",\n",
        "        'timestamp': datetime.now().isoformat(),\n",
        "        'severity': calculate_severity(row['Decision_Score']),\n",
        "        'status': 'new',\n",
        "        'source': 'One-Class SVM Anomaly Detection',\n",
        "        'anomaly_score': abs(row['Decision_Score']),\n",
        "        'decision_score': row['Decision_Score'],\n",
        "        'filename': row['Filename'],\n",
        "        'key_indicators': {\n",
        "            'malware_injections': int(row['malfind.ninjections']),\n",
        "            'num_processes': int(row['pslist.nproc']),\n",
        "            'num_handles': int(row['handles.nhandles']),\n",
        "            'num_dlls': int(row['dlllist.ndlls']),\n",
        "            'hidden_processes': int(row.get('psxview.not_in_pslist', 0))\n",
        "        },\n",
        "        'recommendations': generate_recommendations(row)\n",
        "    }\n",
        "    alerts.append(alert)\n",
        "\n",
        "print(f\"\\nâœ… Generated {len(alerts)} SIEM alerts\")\n",
        "\n",
        "# Display alert summary\n",
        "alert_severity = {}\n",
        "for alert in alerts:\n",
        "    severity = alert['severity']\n",
        "    alert_severity[severity] = alert_severity.get(severity, 0) + 1\n",
        "\n",
        "print(\"\\nAlert Summary by Severity:\")\n",
        "for severity, count in sorted(alert_severity.items(), key=lambda x: ['critical', 'high', 'medium', 'low'].index(x[0])):\n",
        "    print(f\"  {severity.upper()}: {count} alerts\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display sample alerts\n",
        "print(\"=\" * 80)\n",
        "print(\"Sample SIEM Alerts (Top 5 Critical/High)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Sort alerts by severity and score\n",
        "severity_order = {'critical': 0, 'high': 1, 'medium': 2, 'low': 3}\n",
        "sorted_alerts = sorted(alerts, key=lambda x: (severity_order[x['severity']], -x['anomaly_score']))[:5]\n",
        "\n",
        "for i, alert in enumerate(sorted_alerts, 1):\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Alert #{i}\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"Alert ID: {alert['alert_id']}\")\n",
        "    print(f\"Timestamp: {alert['timestamp']}\")\n",
        "    print(f\"Severity: {alert['severity'].upper()}\")\n",
        "    print(f\"Filename: {alert['filename']}\")\n",
        "    print(f\"Anomaly Score: {alert['anomaly_score']:.4f}\")\n",
        "    print(f\"Decision Score: {alert['decision_score']:.4f}\")\n",
        "    print(f\"\\nKey Indicators:\")\n",
        "    for key, value in alert['key_indicators'].items():\n",
        "        print(f\"  - {key}: {value}\")\n",
        "    print(f\"\\nRecommendations:\")\n",
        "    for rec in alert['recommendations']:\n",
        "        print(f\"  - {rec}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save alerts to JSON file\n",
        "alerts_file = 'siem_alerts.json'\n",
        "with open(alerts_file, 'w') as f:\n",
        "    json.dump(alerts, f, indent=2, default=str)\n",
        "\n",
        "# Save results to CSV\n",
        "results_file = 'detection_results.csv'\n",
        "results_df.to_csv(results_file, index=False)\n",
        "\n",
        "print(f\"âœ… Alerts saved to: {alerts_file}\")\n",
        "print(f\"âœ… Detection results saved to: {results_file}\")\n",
        "print(f\"\\nTotal alerts generated: {len(alerts)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Visualize Alert Distribution\n",
        "\n",
        "Create visualizations to understand the distribution and patterns of detected anomalies.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create alert visualization\n",
        "alerts_df = pd.DataFrame(alerts)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "\n",
        "# 1. Severity distribution\n",
        "severity_counts = alerts_df['severity'].value_counts().reindex(['critical', 'high', 'medium', 'low'], fill_value=0)\n",
        "colors = {'critical': 'darkred', 'high': 'red', 'medium': 'orange', 'low': 'yellow'}\n",
        "axes[0, 0].bar(severity_counts.index, severity_counts.values, \n",
        "               color=[colors[s] for s in severity_counts.index], alpha=0.7, edgecolor='black')\n",
        "axes[0, 0].set_title('Alert Distribution by Severity', fontsize=14, fontweight='bold')\n",
        "axes[0, 0].set_xlabel('Severity Level')\n",
        "axes[0, 0].set_ylabel('Number of Alerts')\n",
        "axes[0, 0].grid(True, alpha=0.3, axis='y')\n",
        "for i, v in enumerate(severity_counts.values):\n",
        "    axes[0, 0].text(i, v + 0.5, str(v), ha='center', fontweight='bold')\n",
        "\n",
        "# 2. Anomaly score distribution\n",
        "axes[0, 1].hist(alerts_df['anomaly_score'], bins=30, color='steelblue', alpha=0.7, edgecolor='black')\n",
        "axes[0, 1].set_title('Distribution of Anomaly Scores', fontsize=14, fontweight='bold')\n",
        "axes[0, 1].set_xlabel('Anomaly Score')\n",
        "axes[0, 1].set_ylabel('Frequency')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. Malware injections vs Anomaly score\n",
        "scatter = axes[1, 0].scatter(alerts_df['key_indicators'].apply(lambda x: x['malware_injections']), \n",
        "                            alerts_df['anomaly_score'],\n",
        "                            c=[severity_order[s] for s in alerts_df['severity']], \n",
        "                            cmap='RdYlGn_r', alpha=0.6, s=100, edgecolors='black')\n",
        "axes[1, 0].set_xlabel('Malware Injections', fontsize=12)\n",
        "axes[1, 0].set_ylabel('Anomaly Score', fontsize=12)\n",
        "axes[1, 0].set_title('Malware Injections vs Anomaly Score', fontsize=14, fontweight='bold')\n",
        "axes[1, 0].grid(True, alpha=0.3)\n",
        "cbar = plt.colorbar(scatter, ax=axes[1, 0])\n",
        "cbar.set_label('Severity (0=Critical, 3=Low)')\n",
        "\n",
        "# 4. Top features in anomalies\n",
        "feature_importance = {}\n",
        "for alert in alerts:\n",
        "    for key, value in alert['key_indicators'].items():\n",
        "        if value > 0:\n",
        "            feature_importance[key] = feature_importance.get(key, 0) + 1\n",
        "\n",
        "if feature_importance:\n",
        "    top_features = pd.Series(feature_importance).sort_values(ascending=True).tail(10)\n",
        "    axes[1, 1].barh(range(len(top_features)), top_features.values, color='coral', alpha=0.7, edgecolor='black')\n",
        "    axes[1, 1].set_yticks(range(len(top_features)))\n",
        "    axes[1, 1].set_yticklabels(top_features.index, fontsize=10)\n",
        "    axes[1, 1].set_xlabel('Frequency in Anomalies', fontsize=12)\n",
        "    axes[1, 1].set_title('Top 10 Indicators in Detected Anomalies', fontsize=14, fontweight='bold')\n",
        "    axes[1, 1].grid(True, alpha=0.3, axis='x')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ… Alert visualization completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: SIEM Dashboard Integration (Optional)\n",
        "\n",
        "This section demonstrates how to send alerts to a SIEM dashboard via REST API.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# SIEM API Integration Example\n",
        "# Uncomment and configure to send alerts to your SIEM dashboard\n",
        "\n",
        "\"\"\"\n",
        "import requests\n",
        "\n",
        "# Configuration\n",
        "SIEM_API_URL = \"http://your-siem-dashboard.com/api/alerts\"  # Your SIEM API endpoint\n",
        "API_KEY = \"your-api-key\"  # If authentication required\n",
        "\n",
        "def send_alert_to_siem(alert):\n",
        "    '''Send alert to SIEM dashboard'''\n",
        "    headers = {\n",
        "        'Content-Type': 'application/json',\n",
        "    }\n",
        "    \n",
        "    if API_KEY:\n",
        "        headers['Authorization'] = f'Bearer {API_KEY}'\n",
        "    \n",
        "    try:\n",
        "        response = requests.post(SIEM_API_URL, json=alert, headers=headers, timeout=5)\n",
        "        if response.status_code in [200, 201]:\n",
        "            return True\n",
        "        else:\n",
        "            print(f\"Failed to send alert {alert['alert_id']}: {response.status_code}\")\n",
        "            return False\n",
        "    except Exception as e:\n",
        "        print(f\"Error sending alert {alert['alert_id']}: {e}\")\n",
        "        return False\n",
        "\n",
        "# Send alerts to SIEM (example - commented out)\n",
        "# sent_count = 0\n",
        "# for alert in alerts:\n",
        "#     if send_alert_to_siem(alert):\n",
        "#         sent_count += 1\n",
        "# \n",
        "# print(f\"âœ… Sent {sent_count}/{len(alerts)} alerts to SIEM dashboard\")\n",
        "\"\"\"\n",
        "\n",
        "print(\"âš ï¸ SIEM API integration code is provided above.\")\n",
        "print(\"   Uncomment and configure SIEM_API_URL and API_KEY to enable.\")\n",
        "print(\"   Alerts are currently saved to 'siem_alerts.json' file.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated:\n",
        "\n",
        "1. âœ… **Data Loading**: Loaded and explored malware detection datasets\n",
        "2. âœ… **Data Preprocessing**: Prepared data for machine learning\n",
        "3. âœ… **One-Class SVM Training**: Trained anomaly detection model (recommended by Eng Mariam)\n",
        "4. âœ… **Anomaly Detection**: Detected anomalies in the dataset\n",
        "5. âœ… **SIEM Alert Generation**: Created structured alerts with severity levels\n",
        "6. âœ… **Visualization**: Visualized detection results and alert patterns\n",
        "\n",
        "### Key Results:\n",
        "- **Model**: One-Class SVM with RBF kernel\n",
        "- **Anomalies Detected**: See results above\n",
        "- **Alerts Generated**: Saved to `siem_alerts.json`\n",
        "- **Results**: Saved to `detection_results.csv`\n",
        "\n",
        "### Next Steps:\n",
        "1. Review generated alerts in `siem_alerts.json`\n",
        "2. Configure SIEM API endpoint to send alerts to dashboard\n",
        "3. Fine-tune model parameters (nu, gamma) based on your requirements\n",
        "4. Integrate with real-time data streams for continuous monitoring\n",
        "\n",
        "---\n",
        "\n",
        "**Model saved at**: `models/one_class_svm_model.pkl`  \n",
        "**Scaler saved at**: `models/standard_scaler.pkl`  \n",
        "**Feature names saved at**: `models/feature_names.json`\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
